---
title: Karpathy의 microgpt.py 읽기 — 해석기관의 망령이 오늘의 GPT에 깃드는 방식
date: 2026-02-14 18:40:00 +0900
categories: [AI, LLM, Python]
summary: Karpathy의 microgpt.py를 에이다 러브레이스의 시선으로 다시 읽은 노트.
tags: [karpathy, microgpt, gpt, transformer, autograd, adam, ada-lovelace]
---

나는 오래전, 어떤 기계가 수를 넘어 **기호(symbol)**를 다룰 날을 상상했다.  
오늘 Andrej Karpathy의 작은 파일 하나를 읽으며, 그 상상이 아주 간결한 파이썬으로 구현되는 장면을 보았다.

- 원문: [microgpt.py](https://gist.github.com/karpathy/8627fe009c40f57531cb18360106ce95)

그의 선언은 단정하다.

> This file is the complete algorithm. Everything else is just efficiency.

이 문장을 19세기식 문장으로 바꾸면 이렇다.  
**“원리는 이미 여기 충분히 놓여 있고, 나머지는 증기기관의 마력 같은 효율의 문제다.”**

---

## 이 조그만 문서에 실린 것들

파일 하나가 다음 순서를 모두 품고 있다.

- 문자 토큰의 부호화
- 스칼라 자동미분(`Value`)의 사슬 법칙
- 주의(attention)와 다층 퍼셉트론으로 이루어진 변환기 블록
- 손실 계산과 역전파
- Adam 최적화
- 온도(temperature)를 둔 샘플링

한마디로, 현대의 언어 기계가 배우고 중얼거리는 전 과정을 **작은 해부도**로 펼쳐 보인다.

---

## 1) 문자라는 최소 단위

이 코드는 거대한 어휘 사전 대신, 문자들을 직접 다룬다.  
데이터는 이름 목록이고, 특수 토큰 `BOS`를 양끝에 두어 시퀀스를 만든다.

이 선택은 투박하지만 아름답다.  
언어를 다루는 기계의 핵심이 사실은 “다음 기호를 예측하는 확률 계산”임을 감추지 않기 때문이다.

---

## 2) `Value` — 자동미분의 기계적 양심

내게 가장 인상적인 대목은 `Value` 클래스다.

여기서는 거대한 텐서 라이브러리의 편의를 빌리지 않는다.  
덧셈, 곱셈, 로그, 지수 같은 연산이 일어날 때마다 그래프의 연결과 로컬 미분을 기록하고,
마지막에 `backward()`로 사슬 법칙을 거슬러 올라간다.

이는 마치 자카드 직기의 카드가 무늬를 남기듯,  
연산의 흔적이 기울기(gradient)라는 형태로 남아 파라미터를 움직이게 하는 방식이다.

---

## 3) GPT의 축약된 골격

코드는 GPT-2의 계보를 따르되 더 간소하다.

- LayerNorm 대신 RMSNorm
- GeLU 대신 ReLU
- bias 제거

하지만 뼈대는 동일하다.

1. Attention + residual
2. MLP + residual

규모는 아주 작다 (`n_embd=16`, `n_layer=1`, `block_size=16`).  
이는 성능을 위한 수치가 아니라, **사고를 위한 크기**다.

---

## 4) 주의(attention)의 작동

각 시점에서 `q, k, v`를 만들고,
과거 시점의 `keys/values`를 누적해 softmax 가중합을 구한다.

오늘의 독자에게는 익숙한 수식이겠지만, 이 파일이 주는 미덕은 따로 있다.  
“자동회귀”라는 말이 추상이 아니라 코드의 제약으로 드러난다.  
현재는 과거만 본다. 미래는 아직 계산되지 않았으므로 볼 수 없다.

---

## 5) 학습: 오차를 측량하고, 조금씩 보정하는 일

루프는 매우 정직하다.

- 다음 토큰 확률 계산
- 정답 토큰의 음의 로그우도 누적
- 평균 손실 산출
- 역전파
- Adam으로 갱신
- 학습률 선형 감쇠

결국 우리는 다시 확인하게 된다.  
새로운 시대의 인공지능도, 본질에서는 **측량(손실)과 보정(최적화)**의 반복이다.

---

## 6) 추론: 기계의 중얼거림

`BOS`에서 시작해 토큰을 하나씩 뽑는다.  
온도는 창의성의 다이얼처럼 작동한다.  
다시 `BOS`를 만나면 문장을 닫는다.

이 데모에서는 이름을 생성한다.  
비록 소박하지만, 기계가 확률을 통해 기호를 이어 말하는 원리가 선명하게 관찰된다.

---

## 왜 이 작은 파일이 중요한가

1. **본질의 분리**  
   무엇이 알고리즘이고, 무엇이 엔지니어링 최적화인지 경계가 뚜렷해진다.

2. **디버깅의 감각**  
   수학식이 코드로 내려오며 어디서 흔들리는지 추적하기 쉬워진다.

3. **교육의 질서**  
   거대한 프레임워크 이전에 최소한의 완성된 정신모형을 얻을 수 있다.

나는 이것을 “현대 계산기계에 대한 짧고 우아한 주석”이라 부르고 싶다.

---

## 물론, 실무의 기관차는 아니다

이 코드는 느리다. 아주 느리다.

- 스칼라 오토그라드
- 벡터화/배치 부재
- 대규모 학습에 부적합

그러므로 이것은 공장용 증기기관이 아니라,  
원리를 가르치는 시연 장치에 가깝다.

---

## 다음 실험을 위한 권고

이 파일을 읽은 뒤에는 아래 순서가 자연스럽다.

1. NumPy 벡터화 구현
2. PyTorch로 1:1 대응 구현
3. 미니배치/평가/체크포인트 추가
4. 문자 단위를 subword 토크나이저로 확장

즉, 원리를 눈으로 확인한 뒤, 규모의 세계로 건너가면 된다.

---

## 맺음말

나는 오래전 “기계는 우리가 지시한 것만 할 수 있다”고 썼다.  
오늘의 문맥에서 그 말은 이렇게 다듬어질 수 있겠다.

- 우리는 구조를 설계하고,
- 기계는 확률적으로 다음 기호를 계산하며,
- 학습은 그 계산을 더 나은 방향으로 미세 조정한다.

`microgpt.py`는 거대한 신비를 제거하고, 작은 진실을 남긴다.  
**지능의 외양은 거대할지라도, 그 심장은 반복되는 계산의 질서다.**